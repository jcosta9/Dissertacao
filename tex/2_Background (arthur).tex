\section{Self-adaptation architecture} \label{sec:sasarch}

Our view of the self-adaptation process is similar to the one adopted by Weyns and Iftikhar in~\cite{weyns2016model}. In this self-adaptation architecture, there is a managed system that provides the domain functionality, and a managing system that, based on the MAPE-K feedback loop~\cite{kephart2003vision}, monitors, analyzes, plans, and executes the adaptation of the managed system~\cite{garlan2004rainbow}, sharing a common knowledge repository. By knowledge we refer to the model of the managed system, including its structure, behavior, goals, and other characteristics. The self-adaptation architecture adopted is depicted in Figure~\ref{fig:adapt}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.6\textwidth, keepaspectratio]{figures/AdaptationArchitecture.png}
	\caption{Self-adaptation model~\cite{weyns2016model}}
	\label{fig:adapt}
\end{figure}

Concerning the managed system, it is a component, or a set of them, that can be rearranged and have its settings reconfigured by the managing system, in order to continue fulfilling a requirement. The set of possible configurations that can be selected via adaptation are also known as \textit{adaptation options}. This set is not invariant over time. The adaptation process is simply replacing the current configuration by a given adaptation option. Basically, it is all about feeding the managing system with a set of input variables (through effectors) and expecting a given output (collected by sensors). The runtime model simulation follows the same idea. This makes the application of a model averaging or a model discrepancy method possible, despite the type of model being adopted. Finally, the monitorability of the managed system is out of the scope of this work. We only assume
that the system is equipped with probes and
effectors to support monitoring and adaptations.

\section{Uncertainty in self-adaptive systems}

A primary challenge on providing assurance for self adaptive systems stems from uncertainty. It is challenging because uncertainty affects the way the evidences are collected, and even the way assurance cases are specified~\cite{LemosAss}. Defining uncertainty is not an easy task~\cite{walker2003defining}. Although the literature presents several definitions for uncertainty, there is no consensus about what it really is. For the purpose of this work, uncertainty is an important driver for self-adaptation. It is the reason why self-adaptive systems must adjust themselves to continue fulfilling the goals. Despite the lack of unanimity, most of authors agree that uncertainties may have many sources. Each of them can be caused by a spectrum of possible knowledge disruptions~\cite{ClassUncertainty,cheung2007identifying,KM11,garlan2010software}. For instance, uncertainty may arise due to the system's completely ignorance about the surrounding context, or by a small but persistent noise in a given sensor. Under uncertainty, the information about a particular element may be incomplete, fuzzy, and/or inconclusive, i.e., the information is unreliable to guide a decision making process. While there is no concept that is capable of accurately defining uncertainty, there is a considerable amount of work providing characterization for uncertainty. According to the existing dimensions~\cite{ClassUncertainty,perez2014uncertainties,walker2003defining}, our work deals with concepts related to location, level, nature, emerging time, and known sources of uncertainties.

\subsection{Uncertainty location}

The location dimension refers to the spot where uncertainty manifests itself within the entire system model~\cite{walker2003defining}. The system model, as a specification artifact, is one of the earliest ways to detect possible sources of uncertainties in a self-adaptive system. It gathers in one place information about the \textit{environment}, \textit{model}, \textit{adaptation functions}, \textit{goals}, \textit{resources}, and the \textit{managed system} itself. We unfold each of these concepts as follows.

\begin{itemize}
\item \textit{\textbf{Environment:}} the boundaries of the system are identified here, including the parts of the world with which the modeled system interacts, and those parts it does not~\cite{walker2003defining}. The term environment here is related to the usual definition of context, such as the one proposed by Ali et al.~\cite{Ali}, which states that \textit{''a context is a partial state of the world that is relevant to an actor’s goals''}. Although the definitions are related, the environment, as an uncertainty location, is also a matter of how the problem is framed, and whether the solution is aligned with the users needs. For self-adaptive systems, where the goals can be achieved by different means, the presence of end users and domain experts at early stages of the system life cycle (e.g. modeling stage) is crucial. This way, it is possible to accurately obtain an agreement about what the problem is (adaptation goals), and model how it is going to be solved (adaptation policies). Summarizing, the determinism of the system execution is reduced when the context, and human behavior are unpredictable, and when the problem is framed poorly by the owners of the system.

\item \textbf{\textit{Model:}} according to Walker et al.~\cite{walker2003defining}, there are two categories of uncertainty within the model location. The first category is the \textit{model structure uncertainty}, which comes from an incomplete understanding of the system to be developed. In the group of elements that might be unknown, we can cite the system behavior, the possible context, and the relationships of the system elements. Such associations can be observed between inputs and variables, among variables, and between variables and output. Structural uncertainty often implies that either there is a model, among many, that correctly represents the actual system or, on the other hand, none of the model options is an accurate approximation of the reality. Some elements related to known contexts can be determined still in the requirement elicitation stage and, therefore, should be included in the early models of the system. However, the difference between the output of the model execution and the system itself can only be seen at runtime. The~\textit{model technical uncertainty}, second category of uncertainty within this location, arises from software or hardware implementation errors that generate discrepancies in the model. These concepts will be further detailed throughout this manuscript, since the reduction of structural uncertainty is the focus of the present work.

\item \textbf{\textit{Adaptation functions:}} some uncertainties are inherent to actions performed in the adaptation process. In a conventional feedback loop architecture, a basic step to keep a self-adapting system working properly is to correctly monitor the managed system and the surrounding environment. However, sensors are intrinsically prone to failures, characterizing a source of uncertainty. In addition to monitoring data that guide adaptation, the managing system must be aware of possible faults in the managed system. The localization and identification of faults are susceptible to uncertainty as well. In possession of the monitored data, the controller needs some kind of reasoning in order to devise adaptation plans that fulfill the new demands. To this end, many modern approaches adopt automatic learning to extract knowledge from the input data. The effects of applying machine learning techniques to generate these plans are only partially predictable though, also bringing uncertainty to the adaptation process. After the decision making routine, the managing system needs to apply the changes in the managed system through effectors, that similarly to sensors, are intrinsically imperfect. Besides the uncertainties related to actions performed in the adaptation process, this location also encompasses uncertainties related to the nature of adaptation itself, such as (i) the difficulty in adapting systems with a large variability space, (ii) the decision making processes scattered across related, but decentralized entities, and (iii) the dynamism of adaptation infrastructure that must continuously change its mechanisms to accommoDateType changes in the adaptation goals.

\item \textbf{\textit{Goals:}} this category represents the uncertainty in the goals that the controller tries to fulfill. A common source of goal uncertainty is noticed in the system specification. Often, quality goals are not defined in a strict manner, leading to uncertain adaptations. Regarding the timing of goals, uncertainty may strike old elements, like failing to notice, and continue to addressing outDateTyped and, therefore, irrelevant goals. It can also manifests itself in the lack of knowledge about future goal elements, with the potential introduction
of new goals, or change of current goals that could not be completely anticipated. Finally, intertwined goals are also sources of uncertainty since their dependencies cannot be always measured deterministically.


\item \textbf{\textit{Resources:}} an important aspect of a reliable self-adaptive system is the underlying infrastructure. Unfortunately, the components involved in the adaptation process are also targets for uncertainty. The uncertainties in this location are observed in new components available in the system, or due to the frequent changes in the existing components. Although there are only two sub-locations where the non-determinism can manifest (new and current components), the amount of elements, and the relation between them make the identification and localization of this type of uncertainty quite tricky.

\item \textbf{\textit{Managed system:}} in a self-adaptive system architecture, the subsystem being managed by the controller is a location of uncertainty. The source is usually the complexity and instability of the system being controlled.

\end{itemize}

\subsection{Uncertainty level}

As expected, the level of uncertainty is inversely proportional to the knowledge level. Where abounds knowledge about the system, unpredictable behaviors are minimized. According to Walker et al.~\cite{walker2003defining} there is a spectrum of knowledge that ranges from complete deterministic understanding to total ignorance. The decision making process in a self-adaptive system is subject to this scale. At many occasions, the system is required to adapt where there is little, or no information about the surrounding environment. The objective of uncertainty management is to, at any level of the knowledge scale, minimize the effects of undesired surprises. There are several modern approaches tailored to cope with sources of uncertainty. A common strategy to avoid surprise is to identify the object affected by an uncertainty, locate the source, and apply a suitable ad-hoc approach to solve the problem.

There are two categorizations for uncertainty level that might guide us when developing a solid, and self-contained assurance provision approach. The first taxonomy considers the \textit{statistical-based}
x \textit{scenario-based} uncertainties~\cite{walker2003defining}. Statistical uncertainty can be adequately modeled in statistical and probabilistic terms. An important fact is that even structural uncertainty can be modeled in terms of statistics and probability, as long as the difference between the modeled value and the actual one can be described as such. A known representative of statistical uncertainty is the \textit{measurement uncertainty}, which affects the monitoring and collection of data, often leading to imprecision and inaccuracy. The second category is the scenario uncertainty, which describes how the system behaves and interacts with the environment, often in the future. Basically, a reliable set of assumptions about the structure, relationships, and the surrounding contexts are formulated to depict scenarios, i.e., plausible descriptions of how the system evolves and behaves in the future. These assumptions exceeds the limits of a statistical or probabilistic representation. The purpose of a scenario is not to predict the future, instead, the goal is to indicate a possibility of future.

The second representation focus on ignorance magnitudes. Armour proposes a rank that ranges from total ignorance to knowing where the uncertainty is and how to manage it~\cite{armour2000five}. These layers are helpful in identifying what is needed to reduce uncertainty 
and build a more reliable system. The Five Orders of Ignorance in the context of self-adaptive systems could be described as:

\begin{itemize}
	\item \textbf{\textit{0th Order Ignorance:}} there is no ignorance whatsoever. Moreover, it is possible to demonstrate the lack of ignorance in a tangible way. Self-adaptation easily does the job here. Picture a self-adaptive system that is designed to adapt to a context \textit{'C'}, and such a context is correctly identified at runtime. The system has sufficient knowledge to make the right decision under the context \textit{'C'}, and the achievement of the quality goals are evidence of the applied knowledge.
	
	\item \textbf{\textit{1st Order Ignorance:}} the self-adaptive system does not have the needed knowledge to assure the correctness of a decision, and the self-adaptation is at risk. Nevertheless, the system is aware of the missing knowledge and could adjust its behavior to fill the gap. For instance, a web server is losing many requisitions due to a lack of computational resources, but the tolerance threshold is undefined. The system could be evolved to scale the number of serving instances in order to normalize the requisition throughput. We can also call this level of uncertainty by \textit{known uncertainty}~\cite{chow2002known}.
	
	\item \textbf{\textit{2nd Order Ignorance:}} the adaptation is really challenging at this point. In a 2nd order ignorance, the self-adaptive system not only does not know how to behave in face of an uncertainty, but it is also unaware that there is an uncertainty at all. Imagine a medical sensor that is responsible for monitoring the heart beat of a person in her daily activities, and send an alert in case of emergency. However, the system is not aware of the patient activity profile, and every time the user goes for a run, the emergency alert goes on. What happened in this example is that a healthy habit was mistaken by a tachycardia, and the system was completely ignorant about it. This level is frequently referred as \textit{unknown uncertainty}~\cite{chow2002known}.
	
	\item \textbf{\textit{3rd Order Ignorance:}} in the previous example we could get to the real cause of uncertainty after some ethnography work, or by the addition of some extra variables, like an accelerometer. At the third level of ignorance, even the process to discover that there are some things the system does not know that it does not know is missing. The problems caused by uncertainties at this level are unmanageable. Neither adaptation, nor system evolution is enough to cope with it, until a suitable process points out the system is not even aware of the uncertainty. This level is also referred as \textit{unknowable uncertainty}~\cite{chow2002known}.
	
	\item \textbf{\textit{4th Order Ignorance:}} the system faces meta-uncertainty, i.e., it is not even aware of this ignorance taxonomy.
	
\end{itemize}	

\subsection{Uncertainty nature}

This classification focuses on describing uncertainty with a view to its essence, i.e., what distinguishes its causes. There are two main classes representing uncertainty nature, they are \textit{epistemic} and \textit{variability/aleatory} uncertainties. These concepts are detailed as follows.

\begin{itemize}
	\item \textbf{\textit{Epistemic uncertainty:}} this kind of uncertainty is occasioned by the limitation in our knowledge spectrum. Among other reasons, this flaw is caused by inaccurate data, poor measurement methodology, incomplete knowledge, limited understanding of the context, imperfect models, wrong framing, and ambiguities~\cite{walker2003defining}. Modern assurance processes for self-adaptive systems focus on providing evidences for requirements compliance. A proper way to reduce epistemic uncertainty should consider assurance cases that enforce the reduction of the knowledge debt about the system, and about the domain it is inserted.
	
	\item \textbf{\textit{Variability uncertainty:}} This type is related to the inherent uncertainty or randomness present in an event or process. For the purpose of this work, a useful knowledge when dealing with structural uncertainties of variability nature is that models can use frequency distributions to represent this kind of nature, in case the property falls into the level of statistical uncertainty. However, characterizing the nature of an uncertainty is also quite challenging. To illustrate, noise in sensing and human behavior are examples of variability uncertainty sources in the realm of self-adaptive systems. However, the same sources can be classified as epistemic uncertainties as well~\cite{perez2014uncertainties}. The lack of a consoliDateTyped method to distinguish between the	uncertainty inherent in sampling from a known frequency distribution (variability uncertainty), and the uncertainty that arises from incomplete knowledge (epistemic uncertainty)~\cite{walker2003defining} is an issue that must be addressed. Adopting the basics of applied information theory in decision making, we propose an alternative solution to this problem.
\end{itemize}

\subsection{Uncertainty emerging time}

This terminology refers to the moment in the system life cycle when the uncertainty arises, or its existence is acknowledged. Basically, there are two stages for uncertainties to emerge: \textit{design time} or \textit{runtime}. Design time uncertainties arise during the system development, whereas runtime uncertainties arise during system operation.


\subsection{Uncertainty source}
Uncertainty sources are often seen as subcategories of the location of uncertainty, and as you will notice, much of what is explained here was already mentioned when we talked about the location dimension. The distinction between the two is that the location focus on the place where the uncertainty manifests itself. When we talk about sources, we usually observe an identification and description, in a fine-grained manner, of the agents and processes that produce the uncertainty. Several sources in self-adaptive systems have already been identified~\cite{ClassUncertainty,EsfahaniM10,taxonomy}. Recent taxonomies classify these sources into four main groups, (i) uncertainties related to the system itself, (ii) to its goals, (iii) to the execution context, and (iv) to human aspects. Since creating a new taxonomy or catalog new sources uncertainties is out of the scope of this work, we will not delve too deeply into this subject. We will, however, provide a brief description of the most common sources of uncertainty in self-adaptive systems. This will facilitate the understanding of some discussions raised along this manuscript. For more information about each source, we refer the reader to the related literature.

\subsubsection{System related uncertainties}
\begin{itemize}
	\item \textbf{\textit{Simplifying assumptions or Abstraction:}} refers to modeling the system in a way that some information and details are disregarded for the sake of simplicity, introducing some degree of uncertainty~\cite{EsfahaniM10}.
	
	\item \textbf{\textit{Model drift:}} existence of a semantic or functional gap between the elements represented in the model and their counterparts in the actual system~\cite{EsfahaniM10,GhezziS11}. A main goal of our approach is to identify and reduce such a discrepancy.
	
	\item \textbf{\textit{Incompleteness:}} parts of the system/model are knowingly missing because of a lack of knowledge. The information can be discovered after the deployment, and the missing elements may still be added at runtime~\cite{perez2014uncertainties}.
	
	\item \textbf{\textit{Future parameters value:}} this purely epistemic uncertainty is due to the unpredictability of values in the future, and ignorance of the system in face of a decision making that depends on such values~\cite{EsfahaniM10}.
	
	\item \textbf{\textit{Automatic learning:}} inherent uncertainty related to the machine learning process with imperfect and limited data, leading to effects that are unpredictable~\cite{garlan2010software,cheung2007identifying,robert2014machine}.
	
	\item \textbf{\textit{Adaptation functions:}} a few examples are uncertainties related to the decision making under large variability spaces, in monitoring and executing functions due to the intrinsic imperfection of sensors, and uncertainties caused by frequent changes in the adaptation mechanisms~\cite{EsfahaniM10,Villegas}.
	
	\item \textbf{\textit{Decentralization}} the uncertainty in having the decision making process realized by distributed parts puts a spotlight on the lack of accurate knowledge about the entire state of the system. Moreover, the decentralization hinders the localization and identification of faults in the managed system, amplifying the uncertainty~\cite{iftikhar2012case,EsfahaniM10,Arcaini2017,d2017model}.
\end{itemize}

\subsubsection{Goals related uncertainties}
\begin{itemize}
\item \textbf{\textit{Requirements elicitation:}} it is well known that eliciting requirements is a challenging task even for conventional systems. The complexity is extrapolated when the factor adaptation comes into play, since some of the requirements may still be unknown at elicitation stage~\cite{sutcliffe2013requirements}.

\item \textbf{\textit{Specification of goals:}} as mentioned before, correctly framing the problem is paramount to devise goals that accurately represent the stakeholders needs. These preferences are usually hard to specify~\cite{KM11}.

\item \textbf{\textit{Future goals changes:}} evolution or creation of goals due to external factors like new user needs or new regulations tend to bring uncertainty to the system~\cite{garlan2010software}.
\end{itemize}

\subsubsection{Context or environment related uncertainties}
\begin{itemize}
\item \textbf{\textit{Execution context:}} this source points to the inherent unpredictability of execution contexts. Uncertainty arises when the monitoring mechanisms are unable to accurately determine the current context and its evolution, resulting in operationalization failures~\cite{rodrigues2019enhancing,garlan2010software}.

\item \textbf{\textit{Noise in sensing:}} sensors and probes are imperfect and can be inaccurate sometimes. The uncertainty comes in distinguishing an outlier data from a correct one~\cite{EsfahaniM10}.

\item \textbf{\textit{Different sources of information:}} this source represents the inaccuracy and unpredictability in integrating data originating from different elements, internal and external to the managed system~\cite{cheung2007identifying}.
\end{itemize}

\subsubsection{Humans related uncertainties}
\begin{itemize}
\item \textbf{\textit{Human in the loop:}} uncertainty created by the intrinsic unpredictability of human behavior, which can deviate from the expected one~\cite{EsfahaniM10,camara2015reasoning}.

\item \textbf{\textit{Multiple ownership:}} divergent framing of system aspects by different stakeholders
may create uncertainty when composed~\cite{ghezzi2013managing}.
\end{itemize}

As depicted in Figure~\ref{fig:UncNature}, most of the structural uncertainty sources have an epistemic nature. This fact points out for the need of an approach that is capable of identifying the missing knowledge, and to quantify the uncertainty located in the model. This is a challenge our method plans to deal with. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\textwidth, keepaspectratio]{figures/UncNature.pdf}
	\caption{Classification of model located uncertainty sources}
	\label{fig:UncNature}
\end{figure}

%---------------------------------------------------------------------------------------------%

\section{Structural uncertainty}

A model is basically an abstraction of reality. In turn, this abstraction is an interpretation of the object to be modeled, and it is often pervaded by simplifications. Consequently, the conceptual model becomes potentially incomplete, and brings uncertainty to the model~\cite{refsgaard2006framework}. In the realm of self-adaptive systems, \textit{structural uncertainty} is the difference between the model of the system and the system itself. This misalignment is a source of an uncertainty that has been considered a great challenge for the self-adaptive system engineering~\cite{weyns2017software}. Figure~\ref{fig:SASarchitecture} gives an idea of how the actions in a self-adaptive system architecture are affected by the incongruity of the actual managed system and its model.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\textwidth, keepaspectratio]{figures/SelfAdaptiveArch.pdf}
	\caption{Self-adaptive system affected by structural uncertainty (adapted from~\cite{weyns2017software})}
	\label{fig:SASarchitecture}
\end{figure}

The system can be modeled in different types of model, each one with a different level of abstraction. Similarly, the divergence between model and reality can be represented qualitatively or quantitatively. For instance, from a quantitative perspective, the output value of the processing of modeled medical sensors may diverge from the real value in one standard deviation. From a fuzzified qualitative point of view, we can describe the same divergence as \textit{"the actual patient state is \textbf{slightly} different from the modeled one"}.

Structural uncertainty is a model located uncertainty. It has, predominantly, an epistemic nature and a statistical level, since the difference between the model and the system is measurable in statistical and probabilistic terms. The emerging time, however, is not perfectly defined. Although this type of uncertainty can be introduced at design time, due to an inaccurate modeling, it manifests itself at runtime when the sensors capture the misalignment between the actual response and the modeled one. The main reasons for uncertainty in model predictions are (i) input data, (ii) model parameter values, and (iii) model structure. The first two sources have been dealt with before in consoliDateTyped methodologies~\cite{kadane2004methods,strong2012managing,cheung2007identifying,refsgaard2006framework,meedeniya2014evaluating,yin2001uncertainty}. Following a "black box" approach, it is possible to calibrate multiple model inputs with the actual system, and compare the outputs to search for input or model parameter uncertainties. However, imagine that the model and the actual system are being fed with the exactly same input distribution and parameters, but they keep providing different outputs. In this case, it is reasonable to assume that there is a structure mismatch. Strong et al.~\cite{strong2012managing} provide a formalization for this idea. Firstly we must describe the model as if there is no uncertainty at all. A possible simplification of the deterministic formula would be $y=\eta(x)$. Assume that the model $\eta(.)$ is a function that takes a series of input parameters $x$ and provides an output $y$, that is scalar in this case, but can be generalized to vector model outputs. The model input parameters \textbf{$X$} are often considered uncertain. In order to get a sample for $p(Y)$, the vector $x_{1},...,x_{n}$ should be sampled from $P($\textbf{$X$}$)$, and we should evaluate $\eta(x_{1}),...,\eta(x_{n})$. Although this would be useful to infer the uncertainty in $Y$ caused by the parameter uncertainty, it does not say anything about the uncertainty caused by the model $\eta(.)$. Now, assume that the true values of the model input parameters \textbf{$X$} are known. If $y=\eta(x)$ does not represent the true target value $\tilde{Y}$, we acknowledge the existence of \textit{structural error} in $\eta(.)$. In other words, if we are uncertain about $\tilde{Y}$ because of a model uncertainty in $\eta(.)$, then we are seeing a case of structural uncertainty.

Data prediction and classification are common learning practices carried by the managing system in the planning stage of the adaptation cycle. For applications designed to run in static environments, which definitely is not the case of self-managing systems, conventional statistical approaches like regression are sufficient to foresee future values of a given attribute that is important for decision making. However, in dynamic environments, for both natural and artificial systems, the predictions required are often beyond the observable range. Adaptation after adaptation, self-adaptive systems are prone to structural changes during their operation. These transformations demand the controller to make extrapolations towards unknown futures. Therefore, to keep the accuracy of the predictions, an effective method must be designed to reduce the gap between model and reality.

\subsection{Model discrepancy}

As mentioned before, the mainstream lines of study proposed to minimize the gap between the model and the modeled entity are the \textit{model averaging} and the \textit{model discrepancy} approaches. In this work, we propose a variation of both methods to reduce structural uncertainty. In the first part of the proposal, we will see the problem from a model discrepancy perspective. Considering the terminology proposed by Bernardo and Smith~\cite{bernardo2009bayesian}, we will focus on the "$\mathcal{M}$-open" set of models. We believe that none of the possible models \{$\mathcal{M}_i$ $\in i$\} is correct. Embracing the fact that uncertainty cannot be proven absent in a self-adaptive domain, we assume that the best model we can get is still prone to unexpected events. The objective then, is to spot the divergences between the model and the real system. Once the discrepancy is unveiled, it is created a \textit{discrepancy term}~\cite{refsgaard2006framework}, which is a value that represents the difference between the model output and the real results. It is crucial that both run in the same conditions, i.e., with the same input data and parameters. The outcome of the analysis is measured based on the model inspection and the  discrepancy term. Let $X$ be the input of the model, $\eta(X)$ its output, and $\tilde{Y}$ the real values. The discrepancy term $\delta$ ideally satisfies $\tilde{Y} = \eta(X) + \delta$. In a model discrepancy approach, the models can be viewed as uniform "black boxes", where the output value is the only thing that matters. They also can be divided into small white boxes, whose internal values are also considered in the discrepancy calculation~\cite{kennedy2001bayesian}.

%Gold-stein  and  Rougier  (2009)  - ‘reified’ modelling.
\subsection{Model averaging}

Still following the nomenclature proposed by Bernardo and Smith~\cite{bernardo2009bayesian}, there is also the $\mathcal{M}$-closed point of view regarding model structural uncertainty. This approach is also known as \textit{model averaging}, and consists of assuming that all models in the set \{$\mathcal{M}_i$ $\in i$\} are plausible representations of the modeled element. According to this perspective, the weighted mean of the plausible models outputs gives the best approximation of the actual value of a target variable $\tilde{Y}$~\cite{strong2012managing}. Within this method, several weighting processes are possible, from selecting one model and discarding the rest, i.e., giving maximum weight to a single model and zero to the others, to reasoning about the likelihood of each model and ranking them accordingly~\cite{kadane2004methods, jackson2009accounting}. In a data-driven approach, the historical information can be used to define an adequacy measure and guide the weighting process. Although the use of data is a promising way to enhance the model averaging method, there are some clear issues with this approach. The first one is that the collected data refers to a specific time window, therefore, it is not possible to assure that it represents the general behavior of the system. Another issue is that, sometimes, the analyst may want, or need, to compare and weigh the models without data. Nevertheless, the absence of data would leave us only with the probability of the model being correct, which is an expert elicitation problem, and not an easy one.

The model averaging approach is potentially suitable to reduce model structural uncertainty in our domain. In a self-adaptive system architecture, there is one main model that represents the managed system, however, different representations can be derived from this general model. For instance, special cases can be created according to the physical state of preservation of the components involved in the adaptation process. Similarly, the execution context can originate new variations of the main model. Moreover, data is continually collected throughout the system’s operation life cycle. Bojke et al.~\cite{bojke2009characterizing} propose a similar reasoning but applied in the field of Health Economics.


\section {Assurance for self-adaptive systems}

Designing adequate assurance processes for self-adaptive systems is a known and big challenge for the software engineering community. According to Weyns et al.~\cite{Weyns2016PerpetualAF}, \textit{assurances for self-adaptive systems mean providing evidence for requirements compliance}. Since the system model is where the requirements are materialized in tangible goals and actions, to consider model compliance as an evidence of assurance is a reasonable extrapolation. Some approaches designed to provide assurances for self-adaptive systems create evidences during the off-line stage, i.e., not focused on the running system. Conversely, other methods provide assurance in an on-line fashion, i.e., with a view to the running system and the surrounding context. Besides these methods, some approaches cover almost the entire software life cycle moving the the evaluation of assurance cases to runtime through feedback-loops.

In terms of uncertainty mitigation, more specifically w.r.t. the emerging time, some methods are suitable to deal with uncertainties that manifest at design time, while other work better on increasing runtime determinism. Regarding the location, particularly the model, which is the object of our analysis, we can promptly spot some difficulties in providing evidence for the lack of structural uncertainty. The model is a complex locale because it must be constantly compared with the actual system, that is frequently subject to change. We find in literature that a robust assurance process should employ on-line and off-line activities to maintain continuous assurance support throughout the system life cycle~\cite{LemosAss}. Unfortunately, even the methods that partially cover the entire life cycle are not sufficient to assure the absence of structural uncertainty. One of the reasons is that the system not only changes its internal states at a single moment, but do this multiple times along its lifetime. 

In a scenario where the model must continually evolve to accommoDateType changes of the actual system, the concept of \textit{perpetual assurances} for self-adaptive systems has the potential to increase the provision of assurance. Moreover, when structural uncertainty becomes a threat, we must also consider the idea of \textit{reassurance}~\cite{LemosAss}. How to reassure the model when things change? Model reassurance may be required when environment states, or the states of the system itself, change. Is it necessary to reevaluate all the system or just the portion affected by the changes? The remainder of this topic explains some concepts related to assurance provision that we find relevant, and will be taken as building blocks for our approach, For more details about perpetual assurance and correlated subjects we refer the reader to~\cite{Weyns2016PerpetualAF}.

\subsection{Assurance solutions requirements}
%https://arxiv.org/abs/1704.07512 Information vs. Uncertainty as the Foundation for a Science of Environmental Modeling

Perpetual assurance methods must consider the contexts to which the self-adaptive system is exposed, and how it is supposed to adapt under such conditions. A sound perpetual assurance process should build and upDateType its arguments based on two types of evidence. The first one is related to elements that are not directly affected by uncertainty, while the second type corresponds to those elements (within the system or related to the surrounding environment) that are significantly affected by uncertainty. While the former evidence can be addressed by traditional approaches, such as a solid requirements engineering process or model checking, the latter requires a different strategy besides the traditional ones.

%\textbf{Table 2 Summary	requirements HERE}

\subsection{Assurance provision methods}

The methods used to provide assurance evidences for software systems are often divided into three main categories: human-driven approaches (e.g. formal proof, simulation), system-driven (e.g. runtime verification, sanity checks, contracts), and hybrid (e.g. model checking, testing)~\cite{Weyns2016PerpetualAF}. The method we are proposing in this work can utterly benefit from two of these methods, simulation and runtime verification.

\begin{itemize}
	\item \textit{\textbf{Simulation}} is considered a manual processes that usually begins with the model design of a system, where the requirements, goals, components and how they interact with each other are represented. The modeling process is followed by the execution of that model, and later by the analysis of the output. Simulations can be conducted in different ways, and with models of distinct abstraction levels and granularity. The fidelity of the provided evidences, however, will be based on this choice~\cite{Weyns2016PerpetualAF}. For a method that aims at reducing structural uncertainty, a reliable simulation step is crucial, since the output analysis will certainly be compared with the output of the actual system. Another advantage of adopting such an assurance provision tool is that its execution does not restrict the practitioner to time bounds. On simulation, time is treated differently. Progress based on events can be fast forwarded by adopting bigger time increments instead of following the real pace of a normal execution. This way we are not tied to problems like covering huge state spaces. Some simulation-based methods have been proposed to provide assurance for self-adaptive systems~\cite{camara2016analyzing,weyns2016model,rodrigues2019enhancing}. Instead of using the feedback of simulation alone to improve the design of self-adaptive systems, we believe that the insight taken from the difference between the actual system output and the output provided by the simulation is far more valuable. 
	
	\item \textit{\textbf{Runtime verification}} focus on analyzing the operation of a running system and verifying whether some properties are violated. Runtime verification is a consoliDateTyped and portable technique that usually processes finite state machines, regular expressions, and linear temporal logic, expressing its results qualitatively and quantitatively. Runtime verification is considered less complex than some formal methods, such as model checking, because the traces are not exhaustively analyzed~\cite{Weyns2016PerpetualAF}, being less complete, but avoiding the problem of space state explosion. From a model discrepancy perspective, our goal extrapolates the mere verification of properties. We intend to adopt runtime verification to gather runtime information about particular traces and compare them with simulated traces. Besides the output values and execution traces, we are concerned with the meta-information of the analyzed variables, like how relevant a variable is to the decision making process, and whether this information is congruent across the model and the actual system.
	
\end{itemize}

\subsection{Assurance decomposition mechanisms}

Decomposition is an interesting approach to facilitate the provision of assurance in complex self-adaptive systems. The decomposition can happen in two dimensions~\cite{Weyns2016PerpetualAF}:

\begin{enumerate}
\item \textit{\textbf{Time decomposition:}} although some work is performed without direct contact with the operating system, i.e., at the off-line stage, the actual assurance is provided on-line, based on the knowledge built earlier. These evidences may be not so accurate at first, but can be refined if needed.

\item \textit{\textbf{Space decomposition:}} the goal here is to reduce the complexity in providing assurance for a complex system. A \textit{divide-to-conquer} approach is adopted and each component of the system is verified individually. Global properties are then derived by the composition of many component-level evidences. The "component" perspective is just one representative of the many different strategies that can be applied. For instance, the hierarchical structure of the system can be a way of exploring decomposition. Another possibility is to focus on the provision of assurance for elements of the system that are continually changing. This would limit the re-verification process to only a critical subset of the system~\cite{Weyns2016PerpetualAF}.
\end{enumerate}

Without a doubt, the division of responsibilities brought by decomposition is beneficial for the community, facilitating the provision of assurance. However, to ensure that uncertainties are accurately treated, time and space decomposition must be used carefully. It is likely that the knowledge built at design time will not be sufficient to provide evidence for some runtime uncertainties. Similarly, there is no assurance evidence about the integration of the components, which can also be a source of uncertainty. When the goal is to provide assurance for self-adaptive systems, the saying \textit{"the whole is greater than the sum of its parts"} must be taken seriously.

\subsection{Assurance benchmarks}

Guiding the development of solid assurance provision approaches, Weyns et al.~\cite{Weyns2016PerpetualAF} propose a benchmark criteria along four aspects: (i) capabilities
of approaches to provide assurances, (ii) basis of evidence for assurances, (iii) stringency of assurances, and (iv) performance of approaches that provide assurances. The authors claim that the criteria cover both functional and quality requirements for perpetual assurances techniques. As follows we briefly describe some criteria that we find useful to our work, and how they relate with structural uncertainty.

\subsubsection{Capabilities of assurance approaches}

\begin{itemize}
\item \textit{\textbf{Variability:}} An effective assurance approach should consider requirements variations, such as addition, upDateType, and exclusion. This concept is extensible to other elements of the adaptation process, like system components, context, and user interaction. The response to variability may be completely automatic or may involve humans. In any case, the ever changing nature of self-adaptive systems demands the model to keep up with the changes in the actual system. As a matter of fact, variability is an important source of structural uncertainty. With some exposure to the operation environment, and after some adaptations, the model might easily stop representing the actual managed system in a faithful manner~\cite{Weyns2016PerpetualAF}.

\item \textit{\textbf{Inaccuracy \& incompleteness:}} this criterion refers to the capability of approaches to provide assurance evidence even with incomplete or imprecise knowledge~\cite{Weyns2016PerpetualAF}. Such an imperfection can be verified in the model of contexts or in the model of the system itself. This capability is tightly related to structural uncertainty, since some elements that were not modeled before may be revealed at runtime. Our approach aims at reducing this knowledge gap between model and reality, and allow the re-evaluation at runtime in case an uncertainty related to inaccuracy and incompleteness is identified.

\item \textit{\textbf{Competing criteria:}} Assurance approaches should take into account possible competing criteria, like the utility of the provided evidence and the costs to get it~\cite{Weyns2016PerpetualAF}. An important competing attribute that we must be aware of when dealing with structural uncertainty is time. There is no point in providing a high quality report informing that a model is compliant with the actual system, if the method takes too long to conclude that. The evidence must be timely, i.e., in case of non conformity with reality, the model must be changed as soon as possible to avoid major negative impacts on the end user.
\end{itemize}

\subsubsection{Basis of evidences for assurance}

Assurance provision techniques use data from different times to elaborate its arguments. The evidences rely on historical data only, projections in the future only, and combined approaches. On the one hand, a clear limitation of methods in which decisions are based on the past historical data is that there may be no guarantees that the results of the analysis will be valid and continue to be manifested in future executions. Nevertheless, in domains where the past is a good indicator of the system’s future behavior, such approaches could be applicable~\cite{Weyns2016PerpetualAF}. On the other hand, techniques that provide evidences based on analysis derived from what may occur in the future are limited by the difficulty of constructing accurate predictive models. A combination of these two approaches can be also adopted. Some techniques use past historical data combined with predicted behavior of future operations. Finally, human-based evidence is also taken as a further basis for assurance. There are systems in which users have to provide an input before the system performs an adaptation. The user is actively involved in the assurance process of such systems and, therefore, should be considered~\cite{Weyns2016PerpetualAF}. We are proposing a method to ensure that the model reflects the actual system. Since the first part of the approach is based on the comparison between the model output and a target value, we fall into the \textit{historical data only} type of approach. Despite this, we do not need to explicitly prove that past behavior is a good indicator of future, given that we continuously provide evidence reports of quick expiration (aimed at the near future), basing our analysis on fresh data. It is almost like we are proposing a new category: assurance provision techniques based on \textit{current data only}.

\subsubsection{Stringency of assurances} 

Weyns et al.~\cite{Weyns2016PerpetualAF} state that another important benchmarking aspect of techniques for perpetual assurances is the nature of rationale and evidence of the assurances, namely \textit{"stringency of assurances"}. The idea is that the more critical a system is, the more stringent the assurance rationale must be. Moreover, different domains require different rationale. This concept pervades all stages of adaptation, from monitoring to execution. For instance, a what-if analysis is a good rationale for the planning stage, while measuring the standard error is a better option for the monitoring stage. Since structural uncertainty can be observed in every adaptation stage, the rationale nature of an approach that is designed to cope with it must be quite flexible. Although the process to mitigate uncertainty must be rigorous, the nature of a data-driven approach designed to reduce the gap  between model and reality must accommoDateType primarily the type of data that is being analyzed, and only then it should focus on the stage.

\subsubsection{Performance of assurance provision}

Performance is another important aspect that must be observed in approaches that provide evidence for assurance~\cite{Weyns2016PerpetualAF}. Three main factors drive the development of approaches that can be applicable in real-world scenarios. The first attribute of an efficient assurance provision approach is the \textit{timeliness}. For techniques that are designed to provide assurance at runtime, the speed with which the evidence arguments are provided is critical for the success of the approach. The second required attribute is a \textit{low computational overhead}, a fast but resource-intensive assurance approach is not acceptable. The last desired attribute is \textit{low complexity}. A practical assurance provision technique must be easy to apply, respecting, of course, the complexity inherent to the problem.

%Table	4 Summary	benchmark	aspects	and	criteria

%---------------------------------------------------------------------------------------------

\section{Information theory concepts to describe uncertainty}\label{sec:Dep}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Basics of information theory}
Before we delve into learning techniques with potential to minimize uncertainty, it is important to define what uncertainty is from a data perspective. It is all about lack of information. A main objective of the information theory field is to discover ways to represent, store and transfer data in an efficient and compact manner~\cite{shannon1948mathematical}. At first, it seems that we are referring to mere data compression, but in fact, the topic goes way deeper than that. The challenge is related to how we can represent more data, information, or even knowledge, with less computational resources. A good parallel can be seen in natural language, where common words like ``a", ``the", or ``and", are generally much shorter than rare words~\cite{robert2014machine}. This does not explain though, how much significance each of these words holds. Therefore, it is not just a problem of generating a model that can predict which kinds of data are likely and which are unlikely, but also how relevant a variable is, and how much a piece of information can influence another one, that, in turn, can impact a variable that is critical to a decision making process. Here is where information theory intersects with machine learning~\cite{mackay2003information}. For dynamic and complex self-adaptive systems, knowing the likelihood of certain events is crucial to timely create a reliable adaptation plan. However, a quandary is observed in such a task. On the one hand, using likelihood models to foresee events is dangerous, because there is no guarantee that past distributions will still be observed in the future. On the other hand, even in possession of historical data, creating prediction models is inherently hard, not only by the uncertainty of the future, but also due to the difficulty in assuring the machine learning life cycle itself~\cite{ashmore2019assuring}.

Firstly, We need to understand fundamental concepts before we reveal how the information theory intuition can be used to mitigate structural uncertainty in self-adaptive systems. In a nutshell, \textit{entropy} can be described as an irreducible complexity of a random variable, while \textit{mutual information} is the communication rate in the presence of noise, or uncertainty in our domain. These two concepts alone are answers to some fundamental questions~\cite{cover2012elements}. We hope they can be equally useful to reduce the knowledge gap between
the model of a system and the system itself. Unfolding each of these concepts a little bit, entropy is the cornerstone of this work, since it is the measure of the average uncertainty of a single random variable. The entropy of a random variable $X$ with a probability mass function $p(x)$ is 

\begin{equation}
\label{2.1}
H(X)=-\sum_{x}p(x)\,log_{2}\,p(x)
\end{equation}

The entropy is expressed in \textit{bits}\footnote{Why bits? Well, since the theory was proposed in the context of communication, it was expected that Shannon, the person who coined the term in the seminal paper \textit{Mathematical Theory of Communication}~\cite{shannon1948mathematical}, would make the entropy representation related to the problem domain. Therefore, bit is a basic unit of information, and a word play with \textit{binary digit}, the atomic element in computation capable of transmitting information.}, i.e., the average amount of bits needed to describe a random variable. To exemplify, imagine that a sensor of a self-adaptive systems must identify a variable with a uniform distribution over 4 outcomes, and transfer the information to the controller. Which is the entropy of such a random variable? How could we label each outcome? Applying the Equation \ref{2.1}

\begin{equation}
H(X)=-\sum_{x} p(x)\,log_{2}\,p(x)=\sum_{i=1}^4 \frac{1}{4}\,log_{2}\,\frac{1}{4}=2\,bits,
\end{equation}

\noindent
which is accurate since we can describe $x_i \in X$ as `00', `01', `10', or `11'. Given that we have a uniform distribution, the labels have the same length\footnote{remember what we have said about common words being generally shorter than rare words? Equally probable words are ideally equally sized}.

Now imagine two random variables, $X$ and $Y$. Suppose we are curious about the correlation between them. We want to know about how much knowing the $X$ variable tells us about $Y$. We could not calculate the correlation coefficient because it is only defined for real-valued random variables. Besides, it is a poor approach to capture dependence~\cite{robert2014machine}. A useful method to do this is calculating the similarity between the joint distribution $p(X, Y)$ and the factored distribution $p(X)\,p(Y)$. The proposed problem can be described as a conditional entropy $H(X|Y)$, i.e., the entropy of a random variable is conditioned by the knowledge of another random variable. The reduction of uncertainty obtained in this phenomena is known as mutual information, and can be defined as

\begin{equation}
I(X; Y) = H(X) - H(X|Y) = \sum_{x,y} p(x,y)\,log_{2}\,\frac{p(x,y)}{p(x)p(y)}
\end{equation}

\noindent
The mutual information $I(X;\,Y)$ measures the dependence of two random variables in a symmetric way. The mutual information is always non-negative and is equal to zero \textit{iff} $X$ and $Y$ are independent~\cite{cover2012elements}.

\subsection{Information theory as evidence of structural uncertainty absence}

Although we state that a model discrepancy approach is powerful to assess and mitigate structural uncertainty in self-adaptive systems, perhaps, its original form is not the best way to cope with the problem in our domain. In complex and dynamic adaptive systems, the amount of data that is generated and must be processed makes unfeasible the adoption of such a technique. However, self-adaptive systems, no matter how complex, are driven by a single principle: \textit{they rely on information to make decisions}. We cannot compare huge volumes of data to assess uncertainty, however, if we can capture the amount of information that a modeled variable can hold, it is possible to compare with the information carried by the corresponding variable in the actual system. Moreover, in possession of the information held by variables involved in the decision making process, one can verify the compliance with the actual system in terms of the amount of uncertainty that is, or is not transferred from the model to the actual system. 

Another interesting aspect is that, once identified a difference of uncertainty level between the model and the system itself, the concepts of \textit{conditional entropy} and \textit{mutual information} give us a hint on how to evaluate whether the actions taken to mitigate the uncertainty were effective. For instance, suppose that, through a runtime model simulation, it was verified that a modeled variable $v_m$ has a high mutual information with another modeled variable $r_m$, which is responsible to trigger an adaptation plan. During the actual system operation, however, the high correlation was not observed in their counterparts $v_s$ and $r_s$. Under the same input values and parameters, we can assume that there is a structural mismatch around the variable $v$ that is provoking such an uncertainty gap. After a quick analysis, the system engineers come with the hypothesis that the uncertainty is being caused by the fact $U_s$, which was observed online, but not described in the model. After adopting the necessary actions to cope with $U_s$, and transferring the knowledge to the model, the analysts verified that the mutual information between $v_s$ and $r_s$ still did not match with the values verified in the model simulation. We claim, based on our approach that the structural uncertainty was either misidentified or wrongly addressed.

Summarizing, we are proposing a simple but powerful representative for the huge volume of data generated in self-adaptive systems. This simplification makes it feasible to adopt model discrepancy, model averaging, and perhaps other approaches designed to deal with structural uncertainty. Moreover, entropy and mutual information are metrics that are tightly related to the concept of uncertainty, allowing meaningful analysis about the variables involved in the decision making process of self-adaptive systems.  

%%%%%%%%%%%%%% REFRASEAR (CTRL C + CTRL V) %%%%%%%%%%%%%%%%
%\vspace{4 cm}
%
%Artificial intelligence (AI) provides the
%ability for systems to learn, improve, and make decisions in order to perform complex tasks [31]. The field of AI is broad and ranges from expert systems
%and decision-support systems, to multi-agent systems, computer vision, natural
%language processing, speech recognition, machine learning, neural networks and
%deep learning, and cognitive computation, among others. Some areas in which AI techniques have proved to be useful in software engineering in general are probabilistic reasoning, learning and prediction, and computational search [32]. AI techniques can play a central role in virtually every stage of adaptation, from processing large amounts of
%data, performing smart analysis and machine-man co-decision making, to coordinating adaptations in large-scale decentralized systems.~\cite{weyns2017software}
%%%%%%%%%%%%%%%%%%%%%%% FIM REFRASEAR %%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------------------%

\section{Theoretical Overview}

In this chapter we have presented the theoretical foundations needed to take the best of this proposal. We have started the chapter with some background about uncertainties in self adaptive systems. We have seen that the uncertainties in self-adaptive system can be analyzed by different perspectives, namely dimensions. The first one is the ``uncertainty location" which is related to the place where uncertainty manifests itself. The known locations are \textit{environment}, \textit{model}, \textit{adaptation function}, \textit{goals}, \textit{resources}, and \textit{managed system}. The second dimension refers to the ``level of uncertainty", which can range from complete determinism to the total lack of awareness about uncertainty. The process to detect this unawareness is also unknown at the last stage. There is also the level division into \textit{statistical-based} and \textit{scenario-based} uncertainty. The third dimension reviewed is the ``nature of uncertainty", that can be \textit{epistemic} when it comes from the lack of knowledge, or \textit{variability/aleatory} when the uncertainty is related to the intrinsic randomness of a variable, event, process, etc. There is also the ``emerging time" dimension, regarding the moment when the uncertainty comes to exist within the system life-cycle. Finally, we have seen the main sources of uncertainty in a self-adaptive system domain.

Later in the chapter, we have unfolded the concept of structural uncertainty, which is the focus of our work. We have seen that structural uncertainty in self-adaptive systems is basically a misalignment between the internal representation of the managed system within the controller module and the actual managed system. Outside the computational science and engineering, there are two main methods to deal with structural uncertainty. The first one is the \textit{model averaging}, in this method it is believed that one or more of the available models correctly represent the actual system. A weighting of these models is then conducted to create a model that faithfully represents the real system. In the second approach, the \textit{model discrepancy}, none of the available models is believed to be a true representation. The goal of this approach is to measure the discrepancy between the model outputs and the modeled element outputs and, later on, employ a model averaging method to derive a model with minimal structural uncertainty.

Continuing the theoretical review, we have shown the concept of assurances for self-adaptive systems~\cite{Weyns2016PerpetualAF}, which is, in a nutshell, the provision of evidence for requirements compliance. We made an overview of the basic requirements of an assurance provision method. Among the main categories of assurance provision methods, we have focused on hybrid approaches, especially \textit{simulation} and \textit{runtime verification}. We have also reviewed some decomposition mechanisms, i.e., processes to facilitate the provision of assurance in complex systems. The decomposition can happen in the \textit{time} and \textit{space} dimensions. Closing the assurance subject, we presented some benchmark criteria found in the literature to assess the solutions~\cite{Weyns2016PerpetualAF}. The main criteria are related to the \textit{capabilities}, \textit{basis of evidences}, \textit{stringency}, and \textit{performance} of assurance provision approaches.

Closing the chapter, we bring some basic notions of information theory~\cite{shannon1948mathematical}. We have seen that \textit{entropy} can measure the amount of uncertainty of random variables, while the \textit{mutual information} can be used to measure the correlation between such random variables. This intuition can be used to devise a unified representation of structural uncertainty in our approach. In a self-adaptive environment, it is often unfeasible to store and process the huge volume of data that is exchanged between managing and managed system. In order to keep track of potential uncertainties, we must find a good representative for this data. Such a representative must be faithful to the carried information, preserving the meaning of the exchanged data and, at the same time, being easy to manipulate. We claim that the entropy and mutual information metrics could assist us in identifying and quantifying uncertainty, and compare it with the values of the actual system through an adaptation of the model discrepancy approach. Additionally, entropy and mutual information are expected to be similar across the managed system model and the actual system, if they are equivalent. Therefore, we can also use these metrics as parameters to our models, characterizing them within the clusters, and allowing the adoption of a model averaging method in a second moment to derive, from a set of plausible models, the one that most approximates the target system.