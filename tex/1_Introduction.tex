% Nature, and particularly biology, has long represented a source of inspiration for new computational intelligence paradigms. Similar to the way the nervous system functioning inspired the development of Artificial Neural Networks (ANN), and the Darwinian theory of evolution motivated the development of Evolutionary Algorithms (EA), the study of the biological immune system (BIS) led to the emergence of Artificial Immune Systems (AIS). Since the early 1990s these new emerging soft computing techniques have already been applied in various domains such as pattern recognition, data analysis, fault diagnosis, anomaly detection, machine learning and optimization



\section{Problem Definition}

%%%% Context %%%%
% blablabla
Cyber-physical systems are systems that integrate software components with physical processes \cite{lee2008cyber}. They are a definite reality in our day-to-day lives, specially in the recent years. It is fair to say that the development of autonomous systems and the Internet of Things has taken us quite near to the future visions we had a few decades ago. Flying cars, for example, were supplanted by unmanned aircraft and autonomous drones, for instance, although these technologies are now used for a variety of applications rather than just passenger transportation. The usage of CPSs is also increasing amongst sophisticated applications, whose domains include self-driving vehicles, smart homes, smart cities, and more.

% safety-critical systems need assurances 
Nevertheless, the complexity inherent of these domains raises some challanges. Unforeseen events, for example, might make the CPS unreliable at runtime, which could have disastrous effects. This is a fact, particularly for safety-critical systems, which are subject to strict safety rules, time limitations, and performance requirements, and whose malfunctions might potentially endanger the user's life. Such systems demand the provision of assurances to guarantee that the system's goals are met during its entire lifespan, from conception to operation. 

% they are hard to achieve
% Modern techniques view formal methods as the most promising means of delivering such evidences \cite{assurances2017}. Model checking, for instance, is a automata-based method tipically run offline to check the fulfillment of goals and properties for all attainable states in the model. Nonetheless, this task can become infeasible in highly complex CPS, since the process can suffer from state-space explosion and the loss of representativeness \cite{2014PerceptionsSOTAV&VCPS}.

The Runtime Verification field \cite{colombo2021runtime} tries to address this problem. One of its techiniques relies on deriving a set of properties from the specification that formally describe how the system should behave. Then, runtime monitors are developed as a means to verify the satisfiability of each property while the CPS is being executed. The so called observers are state machines capable of performing the verification task by reading the log files, or the signals sent between the system modules. When a violation is spotted, an error state is reached. Nevertheless, in some cases, it might not be possible to wait for the CPS to be deployed and realize the verification afterwards, since it may potentially be costly or even harmful. The alternative, then, is to develop a model-based prototype of the system \cite{mohamed2021model}. In this case, the CPS would be modeled in a simulation environment, while still in the early stages of development, with the Runtime Verification monitors acting as observers to analyze the properties during the simulation.

However, the CPS verification task is a difficult undertaking yet to be perfectly achieved \cite{patankar2020safety}. One of the reasons is that, when applications are first designed, the knowledge about the environment in which they will be deployed may be incomplete, and also subject to continuous change over the application's lifetime \cite{assurances2017}. This fact may affect the quality aspects of CPSs operation both at a physical and at a logical level, such as a freezing temperature affecting a sensor's capacity to deliver reliable data. Besides that, the complex relation between the cyber and physical aspects of the CPS contributes to the challenging task of monitoring such systems. While the physical process are modeled by acausal time-continuous equations, the discrete behavior of software components can be described as state machines \cite{Baheti2019CyberPhysicalS}. Other complexities are related to the dynamic behavior of the physical components, i.e. deterioration and malfunctioning, for instance.

All of that may cause the observers to miss out on some important aspects that should be considered when monitoring a property. In face of a faulty scenario, the system analyst might wonder "what caused the property violation?" or "which components or behaviors are related to violations of the property?". The intricate relation between the cybernetic and physical natures of CPS creates obstacles when obtaining the set of configurations or behaviors that lead the system to anomalous behavior. Trying to address this problem by debbuging the prototype can be a very time-consuming task, whithout any guarantees of solving it.

Hence there is a need for an approach that increases the reliability of Cyber-physical systems by enhancing the runtime monitors. This technique should support the system analyst in the understanding of the context variabilities that provoke the violation of the system's properties, in a way that accounts for the complexities of the CPS. In summary, the goal of this study can be condensed in the following research question:

\vspace{0.5 cm}

\noindent\fbox{
	\parbox{\textwidth}{
		\textbf{RQ:} How to enhance runtime monitors accurately and systematically to account for the complex interaction between the physical and cybernetic nature of CPS?
	}
}
\vspace{0.3 cm}


% Besides that, the complexities of representing both the cyber and physical aspects of any CPS challenges the task of modeling such systems. Oversimplified models may be invalidated for not anticipating failures dependant on the two layers \cite{2014PerceptionsSOTAV&VCPS}. Often times, the verification mechanism well depicts either the cyber or the physical process, but not both. Physical processes, for instance, are modeled by differential equations while the discrete behavior of software components can expressed by state machines \cite{Baheti2019CyberPhysicalS}. Even though this two-fold segmented modeling may suffice for the development of CPSs in some cases, it creates a significant challenge for verifying the safety and correctness of the overall system and the physical-behavioral interactions at a component level.

% In an ideal scenario, developers should be granted complete knowledge of the system before entering the implementation phase. This would allow the verification at design time through the examination or the anticipation of the potential context and system configurations. 
% Moreover, even in an ideal scenario, where developers where granted complete knowledge of the system before the implementation phase, questions like "what caused the property violation?" or "which components or behaviors are related to violations of the property?" are still difficult to address. The intricate relation between the cybernetic and physical natures of CPS creates obstacles when eliciting the set of configurations or behaviors that lead the system to anomalous behavior. Hence, a correct simulation of the CPS does not suffice to help the system analyst enrich the verification process, and thus, increase the reliability of the system.


% In an ideal scenario, developers should be granted complete knowledge of the system before entering the implementation phase. This would allow the verification at design time through the examination or the anticipation of the potential context and system configurations.
% In reality, both scenarios are impractical since there are too many potential outcomes to predict. Additionally, because the information needed is only available at runtime, it becomes challenging to forecast from the earliest phases of development. All of this can lead to inaccurate estimations of a system's reliability \cite{2014PerceptionsSOTAV&VCPS}.

% Hence, there is a need of new ways to address these issues, that account for the complexity of performing verification in these CPSs in a way that increases the reliabitiy of the system while still in the early phases of development.
% The high demand of safety and reliability in CPS' domains also makes it essential not only to detect but also to isolate and identify faults and abnormalities as early as possible. A system fault can be described as a deviance from the acceptable or usual state of at least one distinctive property or parameter of the system \cite{gao2015survey}. A worth mentioning example is the violation of a system property, which was derived from the requirements and describes what the system should do \cite{2008PrinciplesModelChecking}. The fault diagnosis can be performed by the Analytical redundancy method, which compares input and output data to prior knowledge of a healthy system. This process allows for the removal of the negative effects from the faulty parts on the system's normal operation, thus, increasing the robustness of the CPS. When performed in the first stages of development, it allows for the implementation of fault-tolerant applications that minimizes potential malfunctionings and dangerous situations.








% Practical Causal Models for Cyber-Physical Systems
% When Cyber-Physical Systems (CPS) are causally involved in accidents or unwanted events, rapid diagnosis of the underlying fault is paramount. On the one hand, this builds public trust in those systems, and on the other hand, this is necessary to prevent similar accidents with identically constructed systems. However, the analysis of faults and the attribution of accountability in CPS [15] is especially hard, because these systems interact with their physical environment, have no clear boundaries, and the necessary knowledge is often spread across different models.


% Manuscript


% All of those issues highlight first that we have to provide assurance measures at both design and runtime. Monitoring and verification are means to collect assurance evidences but they require a precise specification of what should be monitored and verified. This is difficult in practice and inherently error prone; humans biases, for instance, can lead to incomplete and oversimplified models and, consequently, undependable systems.
% Monitoring events and exchanging alert messages can be rather costly in a CPS environment,
% which is often resource constrained in terms of
% battery, CPU, memory, and storage [3]. For instance, an unplanned monitoring process that tries
% to supervise all the components in detail, exhausting resources to exchange irrelevant data, may
% have devastating impacts on the battery consumption of the CPS. To avoid such an overhead, the
% system instrumentation must be wisely designed
% so that important information is collected for
% an accurate and dependable assurance evidence
% provision. There are several approaches targeting
% such purpose [4]. However, a method with the
% ability for becoming technology independent and
% maintaining its efficacy is still missing. More
% importantly, the need for monitoring events that
% are not fully known, makes the challenge even
% greater and demands solutions to pave the road
% for engineering safe CPSs.
% In this article, we present a methodology to
% increase the ability to confidently and systematically monitor and analyze CPSs. The methodology combines learning techniques with different
% assurance provision methods in order to account
% for contextual uncertainties that could impair the
% dependability of the CPS. Our methodology is
% better explained with an Artificial Immune System (AIS) metaphor. An AIS is an abstraction
% based on the underlying theory of immunology
% and adopted to solve different computational
% problems. Basically, real immune systems rely
% on “good cells” to detect and combat foreign
% particles. The immune system, for instance, produces T cells, which are triggered by specific
% antigens in order to combat certain anomalies
% (pathogens). Analogously, the methodology we
% are proposing, produces useful observer automata
% similar to T cells. They are mapped to monitors at
% runtime to detect anomalies caused by contextual
% uncertainties in the CPS operation. Intuitively,
% observers are automata designed to reach a certain
% bad state if and only if some property can be
% violated in the observed system model. Our goal
% is thus to guide developers in designing effective
% runtime monitors derived from observer automata
% that account for uncertain execution contexts.

% \cite{seams2018}
% The assurance of real-time properties is prone to context variability. Providing such assurance at design time would require to check all the possible context and system variations or to predict which one will be actually used. Both cases are not viable in practice since there are too many possibilities to foresee. Moreover, the knowledge required to fully provide the assurance for self-adaptive systems is only available at runtime and therefore difficult to predict at early development stages. Despite all the efforts on assurances for self-adaptive systems at design or runtime, there is still a gap on verifying and validating real-time constraints accounting for context variability.
  
% The idea of borrowing concepts from other areas is not restricted to AIS.
% Seeking inspiration in processes from other fields is a very common activity in Computer Science. Specially when it comes to looking at biology


% Causality in Configurable Software Systems
% Configurable software systems offer a wide variety of configuration options that control the activation of features desired by the user and that influence critical functional and non-functional properties such as performance. The often huge configuration spaces render the detection, prediction, and explanation of defects and inadvertent behavior challenging tasks. While there are specifically tailored analysis methods to tackle this challenge [86], research on their explainability is still in its infancy [8]

%%%% Objective %%%%
% description of the main goals.
% Our proposed work aims at supporting the provision of such evidence with special focus on real-time properties of SAS.



%%%% Method %%%%
\section{Proposed Solution}

Seeking inspiration in processes from other fields is a very common activity in Computer Science. Nature, especially biology, has long served as a fruitfull source of methodologies like artificial intelligence approaches. Artificial Neural Networks (ANN) are inspired by the functioning of the nervous system, Evolutionary Algorithms (EA) were based on the Darwinian theory of evolution, and the biologic immune system led to the creation of Artificial Immune Systems (AIS). These new developing soft computing approaches have already been employed in a variety of disciplines since the early 1990s. 

The Negative Selection Algorithm \cite{ICBook2009}, for example, is an immuno-based technique with multiple successful applications in the field of CPS, primarily in the field of fault diagnostics for the identification of anomalous behavior. This one-class method works by developing detectors aimed at classifying data that corresponds to abnormal behavior. A carefull study of these detectors may lead to explanations about the patterns that generate such behaviors and the isolation of the fault and the enabling of the development of fault-tolerant mechanisms that increase the runtime montioring of CPSs. This algorithm may bring several benefits for the field of CPS verification. By defining as abnormal behavior situations where the formalised system properties are not met, the patterns that define violations could be assessed and, hence, the system properties could be enhanced. 

In this work, we propose a methodology that aims at increasing the reliability of CPSs. This is achieved by a systematical diagnosis of system properties violations based on data generated by a protoype, performed in the early stages of development. The immuno-inspired algorithm called Negative Selection (NSA) serves as an analytical redundancy method to indicate the features or behaviors that account for property violations in the system. We believe that, by reasoning about why the property violations happen, the system specification and the property themselves may be refined, fault-tolerant mechanisms may be added, and, thus, safer and better applications might be written.

The methodology is comprised of two main phases. In the first phase, a prototype of the system is implemented based on the specification of the system and a set of properties derived from it. The observers are also implemented in the prototy for the verification of the properties satisfiability during the simulantions. After that, several simulations are performed with varying configurations, input data and parameters in order to account for the context variability that may arise in runtime. 


% the system specification in the form of a Contextual Goal Model (CGM) is used as input to model the behavior of the system as timed automata. Then, by using pattern catalogs, real-time system properties are derived from the CGM and expressed as observer automata. The system model along with the observers, then, undergo a model checking process to assert the system's correctness and the satisfiability of the properties. Finally, 

The second phase focus on the analysis of property violations in the data extracted from the prototype. The data, initially, is feature engineered and labeled as self or nonself, meaning regular data traces or faulty traces that incur in violation. Then, the NSA is executed by generating random detectors based on the features of the dataset. These detectors go through a censoring process, in which they are tested against the self data and are discarded if matched. The resulting set contains detectors specialized in matching nonself data which are carefully examined so that the patterns discovered can be comprehended. This process shares some similarities with the Analytical Redundancy Method \cite{gao2015survey}, since the pre-knowledge of a healthy system are the properties being verified against the prototype data. Hence, it allows for the reasoning about the causes of such violations, which can be used by the Analyst to enhance the sytem's specification, properties and the observers.

The Biological immune system (BIS) not only provides the NSA technique, but also acts as a metaphore for the entire methodology. The initial phase can be related to the Innate Immunity response, since it is the first line of defense of the body. The prototype with the "naive" observers face the context variability without a refined knowledge, just as the nonspecific process of the human body. The antigens are seen here as the execution traces that incur in property violation. The operation data from the simulation follows to the second phase of our approach, which can be compared to the Adaptive Immunity response of the BIS. The biological system produces T cells, which are matured in the Thymus by a censoring process called Negative Selection, in which the lymphocytes are discarded if they strongly bind to proteins of the body. These matured white blood cells are then used by the body to combat pathogens. Analogously, the framework we are proposing uses the Negative Selection algorithm to create a set of random detectors, which are "matured" by being tested against the operation dataset from the prototype. These detectors, in turn, are used to enhance observer automata capable of acting with a more refined knowledge of the environment. 

\section{Evaluation}

We experimentally assess our strategy by designing a version of the Body Sensor Network (BSN) \cite{2021BSN}. Its Contextual Goal Model specifies a set of modules and resources, like sensors, battery and patient's sensed data, that will allow us to represent it as a Cyber-Physical system. A system prototype of the CPS will be implemented in a simulation environment called OpenModelica \cite{OpenModelica}, which is a well known graphic modeling tool for the development of CPS prototypes in the Modelica language \cite{Modelica}.
The simulations were performed by using a dataset of 1,000 randomly generated patient data. The feature engineering process and the Negative Selection Algorithm were both implemented in the Python programming language, which was also used in the causality analysis of the detectors.

The NSA was evaluated regarding its generalization to unseen data. In this matter, all of the execution segments that the model identified as property violations found were, in fact, violations, accounting for a precision of 100\%. Nevertheless, some patterns were not detected by the algorithm, resulting in a recall rate of 0.9958. In addition, the analysis of the efficiency of the technique was performed based on the number of patterns found. The generated nonself detectors resulted in the discovery of 49 unveiled patterns related to faulty segments, which were grouped into 11 clusters based on similarity. 

\section{Organization}
The rest of the document is organized as follows. Chapter \ref{2_Background} introduces some concepts that are useful for the understanding of the framework. In Chapter \ref{sec:BSN} introduces the case study that will be followed along the work. Chapter \ref{4_Proposal} futher details the proposed approach. Chapter \ref{3_RelatedWork} show some works that are strongly related to our proposed methodology. Chapter \ref{5_Evaluation} presents the results of the work after the evaluation, while Chapter \ref{sec:Conclusion} concludes and provides the directions for future works.